{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":5405554,"sourceType":"datasetVersion","datasetId":3131652},{"sourceId":13146295,"sourceType":"datasetVersion","datasetId":8329044}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"colab":{"provenance":[]}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# importing essential libraries\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\n%matplotlib inline\npd.set_option('display.max_columns', None)\n# from google.colab import drive\n# drive.mount('\\content\\drive')/kaggle/input/praacticingrnn\n\nraw_df = pd.read_csv('/kaggle/input/processed-data-credit-score/Score.csv', low_memory= False) # low_memory\n\ndf = raw_df.copy() # creating a copy of the original dataset.","metadata":{"id":"uun4MeCSid2b","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.head()# checking the top five rows","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":556},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1745417714678,"user":{"displayName":"Tajwinder Singh","userId":"00752918548403413662"},"user_tz":-330},"id":"-MqNsJSVoIxB","outputId":"76f7ce4b-92a2-4628-c2da-64e8aa6a67af","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(df.info()) # getting the information of the data","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1069,"status":"ok","timestamp":1745417715741,"user":{"displayName":"Tajwinder Singh","userId":"00752918548403413662"},"user_tz":-330},"id":"LoY0EwNloU5N","outputId":"227b7233-ba74-40b9-dca5-11026e70cfb9","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.isnull().sum() # checking the sum of null values","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":899},"executionInfo":{"elapsed":23,"status":"ok","timestamp":1745417715741,"user":{"displayName":"Tajwinder Singh","userId":"00752918548403413662"},"user_tz":-330},"id":"D4ZkXJtqodKC","outputId":"eb7c5222-3891-4645-8f74-c237b0fcf85d","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# capturing the numerical features\nnum_features = [feature for feature in df.columns if df[feature].dtypes != 'O']\nprint('Numerical feature: ',num_features)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19,"status":"ok","timestamp":1745417717542,"user":{"displayName":"Tajwinder Singh","userId":"00752918548403413662"},"user_tz":-330},"id":"ZKVpggtmszqG","outputId":"d94d7f61-6a11-4087-dbb2-cbdb31fab410","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# capturing the categorical features\ncategorical_features = [feature for feature in df.columns if df[feature].dtypes == 'O']\nprint('categorical features: ', categorical_features)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17,"status":"ok","timestamp":1745417717542,"user":{"displayName":"Tajwinder Singh","userId":"00752918548403413662"},"user_tz":-330},"id":"0QN7eZheT_-n","outputId":"6ac644bc-fe4b-48e5-c7dc-04f656e8ac5b","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# capturing discrete features\ndis_features = [feature for feature in num_features if df[feature].nunique() <= 25]\nprint('discrete features: ',dis_features)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16,"status":"ok","timestamp":1745417717543,"user":{"displayName":"Tajwinder Singh","userId":"00752918548403413662"},"user_tz":-330},"id":"zroEZqpeU2RY","outputId":"5be314b5-c910-4416-c474-d880edbcbee8","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# capturing continuous features\ncon_features = [feature for feature in num_features if df[feature].nunique() > 25]\nprint('continuous features: ',con_features)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1745417717543,"user":{"displayName":"Tajwinder Singh","userId":"00752918548403413662"},"user_tz":-330},"id":"s1HIaPkSVa07","outputId":"f321e6d9-e338-4292-a6b7-66743e93feff","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# checking the number of unique categories in categorical features\nfor feature in categorical_features:\n  print(\"Number of unique categories in: '{}' feature are: {}\\n\".format(feature, df[feature].nunique()))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1745417717543,"user":{"displayName":"Tajwinder Singh","userId":"00752918548403413662"},"user_tz":-330},"id":"PHPyEsZrXHxf","outputId":"f868fc89-004d-4dca-c4f0-14a8c1b16a8e","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Doing train test split by considering the 'Credit Score' feature as an output feature.","metadata":{"id":"laW4F8TlrQdQ"}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split # 70% training, 15% validation, and 15% test.\n\n# First-step: train + test\nX_train, X_temp, y_train , y_temp = train_test_split(df.drop(['Credit_Score', 'Amount_invested_monthly', 'Monthly_Balance', 'Credit_Utilization_Ratio'],axis=1) ,df['Credit_Score'], test_size = 0.3, random_state= 42)\n\nX_test, X_val, y_test, y_val = train_test_split(X_temp, y_temp, test_size = 0.5, random_state = 42)\n","metadata":{"id":"_d1APnqJrQde","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train.head()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_val.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_test.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(X_train.info())\nprint(X_val.info())\nprint(X_test.info())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# checking the number of unique categories in categorical features\ncat_features = [feature for feature in X_train.columns if X_train[feature].dtypes == 'O']\nfor feature in cat_features:\n  print(\"Number of unique categories in: '{}' feature are: {}\\n\".format(feature, X_train[feature].nunique()))","metadata":{"_kg_hide-output":true,"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"numericalCols = [feature for feature in X_train.columns if X_train[feature].dtypes != 'O' ]\nprint((numericalCols))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Log Transformation","metadata":{}},{"cell_type":"code","source":"# capturing right skewed features:\nskewedFeatures = [feature for feature in X_train.columns if X_train[feature].dtypes != 'O' if (X_train[feature] > 0).all() if X_train[feature].nunique() > 25]\nprint('Features that are right skewed are: {}'.format(skewedFeatures))\n\n\n# Creating a new dataframe where transformed x train and test will be stored.\nXtrainLog = X_train.copy() \nXvalidateLog = X_val.copy()\nXtestLog = X_test.copy()\n\n\n# Log transformation on Xtrain and Xtest\nfor feature in skewedFeatures:\n    XtrainLog[feature] = np.log1p(XtrainLog[feature])\n    XvalidateLog[feature] = np.log1p(XvalidateLog[feature])\n    XtestLog[feature] = np.log1p(XtestLog[feature])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"XtrainLog.columns","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"XtrainLog.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"XvalidateLog.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Feature Scaling","metadata":{}},{"cell_type":"code","source":"# Using MinMaxScaler for feature scaling which will convert the value between 0 to infinity. Scaling only Numerical features.\nfrom sklearn.preprocessing import MinMaxScaler\n\nX_train_scaled = XtrainLog.copy()\nX_val_scaled = XvalidateLog.copy()\nX_test_scaled = XtestLog.copy()\n\n\nscaler = MinMaxScaler()\n\nX_train_scaled.loc[:,numericalCols] = scaler.fit_transform(X_train_scaled.loc[:,numericalCols])\n\nX_val_scaled.loc[:, numericalCols] = scaler.transform(X_val_scaled.loc[:, numericalCols])\n\nX_test_scaled.loc[:,numericalCols] = scaler.transform(X_test_scaled.loc[:,numericalCols])\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Before moving a head, I am training an XGBoostClassifier in order to check the most important/top-best features that will also be used to train the ANN. This will also help to find us whether training the simple XBG will give good accuracy as compared to ANN or not.","metadata":{}},{"cell_type":"code","source":"# Performing OHE as it is needed for XGB instead of Label Encoding.\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn.compose import ColumnTransformer\n\n\nct = ColumnTransformer(transformers = [('cat_enc', OneHotEncoder(drop = 'first', handle_unknown = \"ignore\"), cat_features)], remainder = 'passthrough')\n\nX_train_scaled_copy = X_train_scaled.copy()\nX_test_scaled_copy = X_test_scaled.copy()\n\nX_train_encoded = ct.fit_transform(X_train_scaled_copy) # sparse = False will return a OHE array which we want.\nX_test_encoded = ct.transform(X_test_scaled_copy)\n\n\n# ------------------------------------\n# Converting this encoded data into a proper data frame\n# ------------------------------------\n# 1️⃣ Get OHE feature names\nohe_feature_names = ct.named_transformers_['cat_enc'].get_feature_names_out(cat_features)\n\n# 2️⃣ Get passthrough (numerical) feature names\npassthrough_features = [name for name in X_train_scaled_copy.columns if name not in cat_features]\n\n# 3️⃣ Combine them in order\nall_feature_names = list(ohe_feature_names) + passthrough_features\n\n# 4️⃣ Convert encoded array back to DataFrame with proper column names\nX_train_encoded_df = pd.DataFrame(X_train_encoded, columns=all_feature_names, index=X_train_scaled_copy.index)\nX_test_encoded_df = pd.DataFrame(X_test_encoded, columns=all_feature_names, index=X_test_scaled_copy.index)\n\n\n# --------------\n# Encoding target variable\n# --------------\ny_encoder = LabelEncoder()\ny_train_enc = y_encoder.fit_transform(y_train)\ny_val_enc = y_encoder.transform(y_val)\ny_test_enc = y_encoder.transform(y_test)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Capturing important features\nfrom xgboost import XGBClassifier\nimport pandas as pd\nfrom sklearn.metrics import accuracy_score\n\n# Train a quick XGBoost model\nxgb = XGBClassifier(\n    n_estimators=200,\n    learning_rate=0.1,\n    max_depth=6,\n    random_state=42,\n    n_jobs=-1\n)\nxgb.fit(X_train_encoded_df, y_train_enc)\n\n# Get feature importances\nimportances = xgb.feature_importances_\nfeat_imp = pd.Series(importances, index=X_train_encoded_df.columns)\n\n# Sort descending\nfeat_imp = feat_imp.sort_values(ascending=False)\n\n# Display ranked features\nprint(\"Feature Importances (XGBoost):\")\nprint(feat_imp)\n\n# Select top N features (say top 15)\ntop_features = feat_imp.head(15).index\nX_train_selected = X_train_encoded_df[top_features]\nX_test_selected = X_test_encoded_df[top_features]\n\n\nprint(\"XGBoost Classifier Accuracy:\", accuracy_score(y_test_enc, xgb.predict(X_test_encoded)))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Now, on these top features I am performing StratifiedKFold. This  will help to know that which top 'n' features that are giving higher accuracy\nfrom sklearn.model_selection import cross_val_score # Cross validation will calculate the accuracy at each cv\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nmax_k = len(feat_imp)\n\nscores = []\n\nfor k in range(1, max_k + 1):\n    topk = feat_imp.head(k).index.tolist()\n    Xk = X_train_encoded_df[topk]\n\n    model = XGBClassifier(n_estimator = 200, max_depth = 6, random_state = 42, n_jobs = -1)\n    score = cross_val_score(model, Xk, y_train_enc, cv = 5, n_jobs = -1, scoring = 'accuracy')\n    scores.append(score.mean())\n\n    print(f\"K = {k:2d} CV accuracy = {score.mean():.4f}\") # 'd' means that a single integer 'K' value will take two-integer space \n\n\n# Plotting accuracy vs k\nplt.figure(figsize = (10, 5))\nplt.plot(range(1, max_k+1), scores, marker = 'o')\nplt.xlabel(\"K-value\")\nplt.ylabel(\"Accuracy Mean\")\nplt.title(\"Accuracy Vs K-Value\")\nplt.grid(True)\nplt.show()\n\nbest_k = np.argmax(scores) + 1 # Capturing the value of the best 'K' by adding 1 with the highest score index\nprint(f\"Highest CV accuracy {max(scores):.4f} at K = {best_k}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Categorical Encoding will be performed using Categorical Embedding Instead of OHE to train ANN:","metadata":{"id":"Kh5FXhybA8RV"}},{"cell_type":"code","source":"# At first using label encoding to convert each category into labelled integerIDs.\nfrom sklearn.preprocessing import LabelEncoder\n\n# Creating encoders\nPayment_of_Min_Amount_enc = LabelEncoder()\nCredit_Mix_enc = LabelEncoder()\nPayment_Behaviour_enc = LabelEncoder()\n\n\n# Fit and Transform. Since LabeEncoder can't handel multiple columns transformation therefore, doing column-wise transformation.\nX_train_scaled['Payment_of_Min_Amount'] = Payment_of_Min_Amount_enc.fit_transform(X_train_scaled['Payment_of_Min_Amount'])\nX_val_scaled['Payment_of_Min_Amount'] = Payment_of_Min_Amount_enc.transform(X_val_scaled['Payment_of_Min_Amount'])\nX_test_scaled['Payment_of_Min_Amount'] = Payment_of_Min_Amount_enc.transform(X_test_scaled['Payment_of_Min_Amount'])\n\n\nX_train_scaled['Credit_Mix'] = Credit_Mix_enc.fit_transform(X_train_scaled['Credit_Mix'])\nX_val_scaled['Credit_Mix'] = Credit_Mix_enc.transform(X_val_scaled['Credit_Mix'])\nX_test_scaled['Credit_Mix'] = Credit_Mix_enc.transform(X_test_scaled['Credit_Mix'])\n\n\n\nX_train_scaled['Payment_Behaviour'] = Payment_Behaviour_enc.fit_transform(X_train_scaled['Payment_Behaviour'])\nX_val_scaled['Payment_Behaviour'] = Payment_Behaviour_enc.transform(X_val_scaled['Payment_Behaviour'])\nX_test_scaled['Payment_Behaviour'] = Payment_Behaviour_enc.transform(X_test_scaled['Payment_Behaviour'])\n\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Checking the number of unique categories in categorical features\n\nfor feature in cat_features:\n  print(\"Number of unique categories in: '{}' feature are: {} and the unique values in each column are: {}\\n\" .format(feature, X_train_scaled[feature].nunique(), X_train_scaled[feature].unique()))\n    ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Now, I am training the model without Hyperparameter Tuning.","metadata":{}},{"cell_type":"code","source":"# Creating embedding layers for categorical features.\n\nfrom tensorflow.keras.layers import Input, Embedding, Flatten, Concatenate, Dense, Dropout, LeakyReLU\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import AdamW\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\nfrom sklearn.utils.class_weight import compute_class_weight\n\n# creating a dictionary to specify output_dimensions of each feature.\nembedding_sizes = {\n    'Payment_of_Min_Amount': 2,\n    'Credit_Mix': 2,\n    'Payment_Behaviour': 3\n}\n\ninputs = [] # To store different input features\nembeddings = [] # Embdding layers will be concatenated\n\nfor cat in cat_features:\n    inp = Input(shape = (1, ), name = cat+'_in') # Here, initially, we specify the shape of each category. This shape indicates that the category's shape is an integer value only. Later, when we fit the model with the data, the model will map each ID(category) according to this input shape that is the shape of an integer.\n\n    inputs.append(inp) # Appending each Categorical feature's input size. Here this shape specify that there will be a single integer ID(label) taken.\n\n    \n    # Embedding layer:\n    input_dim = X_train_scaled[cat].nunique()\n    out_dim = embedding_sizes[cat] # It will capture the output dimensions of each feature\n    emb = Embedding(input_dim = input_dim + 1, output_dim = out_dim)(inp) # creating embedding layer of the feature. (inp) will map each ID and create a dimensional vector for the ID .i.e., the category.\n    emb = Flatten()(emb) # This will make the 3D shape of layer into 2D. \n    embeddings.append(emb)\n\n\n# Numerical Input\nnum_in = Input(shape = (len(numericalCols), ), name = 'numerical_in')\ninputs.append(num_in)\n\n\n# Concatenating embeddings and inputs: Here, ()() works because Concatenate and Dense are two classes and they can call each other in this way. 'x' stores the entire information in the computational graph and passes the information to the output layer.\nx = Concatenate()(embeddings + [num_in]) # This will concatenate the embeddings and numerical features. This concatenation as I said earlier, will be visible when the data gets fit.\n\n\nx = Dense(512, activation = 'relu', kernel_initializer = 'he_uniform')(x)\nx = Dropout(0.2)(x)\n\nx = Dense(256, activation = 'relu', kernel_initializer = 'he_uniform')(x)\nx = Dropout(0.2)(x)\n\nx = Dense(256, activation = 'relu', kernel_initializer = 'he_uniform')(x) \nx = Dropout(0.2)(x) \n\nx = Dense(128, activation = 'relu', kernel_initializer = 'he_uniform')(x) \nx = Dropout(0.2)(x) \n\nx = Dense(64, activation = 'relu', kernel_initializer = 'he_uniform')(x) \nx = Dropout(0.2)(x)\n\n\n# Creating Output layer\noutput = Dense(3, activation = 'softmax', kernel_initializer = 'glorot_uniform')(x)\n\n\nmodel = Model(inputs = inputs, outputs = output) # You know about why we pass inputs. But, we are passing output here, to specify that the final output of the model should be considered from this output layer itself.\nmodel.compile(optimizer = 'adam',\n              loss = 'sparse_categorical_crossentropy',\n              metrics = ['accuracy']) # Accuracy can also be seen when during the output of every epoch\n\n\n# ----------------------------------------------\n# Preparing Input For Data Training. Now, here, we are specifying the value for each input which we have specified in 'num_in' and 'inputs'. \n# ----------------------------------------------\n\n# Capturing input labels of each column\ntrain_cat_inputs = [X_train_scaled[cat].values for cat in cat_features]\nval_cat_inputs = [X_val_scaled[cat].values for cat in cat_features]\n\n# Capturing values of each numerical feature\ntrain_num_input = X_train_scaled[numericalCols].values\nval_num_input = X_val_scaled[numericalCols].values\n\n\n# Combining all inputs in the same order as 'inputs' list\ntrain_model_inputs = train_cat_inputs + [train_num_input]\nval_model_inputs = val_cat_inputs + [val_num_input]\n\n# Early Stopping\nes = EarlyStopping(\n    monitor = 'val_loss',\n    patience = 10,  # If validation loss stops decreasing till 15 rows then, the training will stops.\n    restore_best_weights = True,\n    verbose = 1\n)\n\n\n# Reducing the Learning Rate if the validation Loss reaches Plateau .i.e., stops improving. This will help in fine tuning the model and improve accuracy by reducing the Learning Rate after waiting for some 'Patience value'.\nreduceLR = ReduceLROnPlateau(\n    monitor = 'val_loss',\n    patience = 8,\n    factor = 0.5, # This will divide the learning rate by 0.5 each time IF the val_loss stops improving.\n    vebose = 1,\n    min_lr = 1e-6 # This is the minimum bound of the Learning Rate which is 0.000001. If the Learning Rate can dicrease till bound only. \n)\n\n\n# Balancing output variable\nclass_weights = compute_class_weight(\n    class_weight = \"balanced\",\n    classes = np.unique(y_train_enc),\n    y = y_train_enc\n)\n\nclass_weights = dict(enumerate(class_weights)) # Creates a dictionary where the key will index of class and value will be weight of the corresponding class.\n\n# Fiting model\nmodel.fit(train_model_inputs, y_train_enc,\n          batch_size = 64,\n          epochs = 200,\n          validation_data = (val_model_inputs, y_val_enc),\n          callbacks = [es, reduceLR],\n          class_weight = class_weights)\n\n","metadata":{"id":"9M06G6P1A8RW","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_train.head(20)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_train_enc[:20]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Hyperparameter Tuning using Bayesian Optimization","metadata":{}},{"cell_type":"code","source":"# We can train hyperparameters like: layer, neurons, LR, embedding dimensions, etc. But, here, tunned only LR since tunning other parameters worsen the accuracy as compared to without tuning.\n\nfrom tensorflow.keras.layers import Input, Embedding, Flatten, Concatenate, Dense, Dropout\nfrom tensorflow.keras.models import Model\nfrom tensorflow import keras\nimport keras_tuner as kt\nfrom keras.callbacks import EarlyStopping\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom functools import partial\nimport numpy as np\n\n\n# ---------------------------------\n# Defining model builder using Bayesian Optimization\n# ---------------------------------\ndef model_builder(hp, cat_features): # Passing 'hp' object and the cat_features as an argument. The 'hp' Hyperparameter is an object of kerastuner\n        # --------------------------------\n    # Preparing categorical embedding and numerical inputs.\n    # --------------------------------\n    categorical_inputs = [] # To store different input features\n    embedding_layers = [] # Embdding layers will be concatenated\n    \n    for cat in cat_features:\n        inp = Input(shape = (1, ), name = cat+'_in') # Here, initially, we specify the shape of each category. This shape indicates that the category's shape is an integer value only. Later, when we fit the model with the data, the model will map each ID(category) according to this input shape that is the shape of an integer.\n    \n        categorical_inputs.append(inp) # Appending each Categorical feature's input size. Here this shape specify that there will be a single integer ID(label) taken.\n    \n        \n        # Embedding layer:\n        input_dim = X_train_scaled[cat].nunique()\n        out_dim = min(50, (input_dim + 1) // 2) # It will capture the output dimensions of each feature with correct formula\n        emb = Embedding(input_dim = input_dim + 1, output_dim = out_dim, input_length = 1)(inp) # creating embedding layer of the feature. (inp) will map each ID and create a dimensional vector for the ID .i.e., the category. Written '+1' because categories are encoded and starting from 0, so, + 1 will to give the correct lenght of the unique cats The input_length = 1 will specify the shape of input as inp does but, mentioning 'input_length' is a good practice.\n        emb = Flatten()(emb) # This will make the 3D shape of layer into 2D. \n        embedding_layers.append(emb)\n    \n    \n    # Numerical Input\n    numerical_input = Input(shape = (len(numericalCols), ), name = 'numerical_in')\n\n    x = Concatenate()(embedding_layers + [numerical_input])\n\n\n    # Creating dense layers\n    x = Dense(128, activation = 'relu', kernel_initializer = 'he_uniform')(x)\n    x = Dropout(0.2)(x)\n    \n    x = Dense(64, activation = 'relu', kernel_initializer = 'he_uniform')(x)\n    x = Dropout(0.2)(x)\n\n    # Output Layer\n    num_classes = len(np.unique(y_train_enc))\n    output = Dense(num_classes, activation = 'softmax')(x)\n\n    model = Model(inputs = categorical_inputs + [numerical_input], outputs = output)\n\n\n    # Tuning learning rate\n    lr = hp.Float(f\"learning_rate\", 0.0001, 0.01, sampling = 'log') # sampling = 'log' ensures the values are taken on a logrithmic scale only betwenn this min-max range.\n\n    # Compiling model\n    model.compile(optimizer = keras.optimizers.Adam(learning_rate = lr), loss='sparse_categorical_crossentropy', metrics = ['accuracy'])\n    \n    return model\n\n# ------------\n# Balancing classes\n# ------------\nclass_weights = compute_class_weight(\n    class_weight = \"balanced\",\n    classes = np.unique(y_train_enc),\n    y = y_train_enc\n)\n\nclass_weights = dict(enumerate(class_weights)) # Creates a dictionary where the key will index of class and value will be weight of the corresponding class.\n\n# ---------------------\n# Bayesian Optimization\n# ---------------------\ntuner = kt.BayesianOptimization( \n    partial(model_builder, cat_features = cat_features), # partial() is an function that helps to send an argument\n    objective = 'val_accuracy',\n    max_trials = 20,     # number of hyperparameter combinations to try\n    directory = 'bayes_tuner',\n    project_name = 'multiclass_ann'\n)\n\n\nes = EarlyStopping(monitor = 'val_loss',\n                   patience = 8,\n                   min_delta = 1e-4,    # Ignores tiny, insignificant improvements of val_loss that don’t matter practically.\n                   restore_best_weights = True,  # Ensures the final model is the one with the highest validation accuracy, not the last epoch.\n                   verbose = 2\n                  \n                  )\n\n\n                   \n# ----------\n# Running Search\n# ----------\ntuner.search(\n    [X_train_scaled[col].values for col in cat_features] + [X_train_scaled[numericalCols].values],\n    y_train_enc, \n    epochs = 10,\n    validation_data= ([X_val_scaled[col].values for col in cat_features] + [X_val_scaled[numericalCols].values], y_val_enc),\n    batch_size = 32,\n    verbose=2,   # this will display output of the accuracy and losss for each epoch\n    class_weight = class_weights,\n    callbacks = [es]\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -----------\n# Getting Best Model\n# -----------\nbest_hyps = tuner.get_best_hyperparameters(num_trials = 1)[0] # This will select top and best performing hyperparameters from a one trial only.\nmodel = tuner.hypermodel.build(best_hyps) # Takes the best hyperparameters and rebuilds the model using them.\n\n\n# Training final model fully for maximum accuracy.\nhistory = model.fit([X_train_scaled[cat].values for cat in cat_features] + [X_train_scaled[numericalCols].values],\n                    y_train_enc,\n                    epochs = 30,\n                    batch_size = 32,\n                    validation_data = ([X_val_scaled[cat].values for cat in cat_features] + [X_val_scaled[numericalCols].values], y_val_enc),\n                    class_weight = class_weights\n                    )\n                    \n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Tuning the Learning Rate was not causing any impact on the validation accuracy. So, the best model is the ANN trained without BO.","metadata":{}},{"cell_type":"markdown","source":" ","metadata":{}}]}